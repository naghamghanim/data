{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naghamghanim/data/blob/main/training1000.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify that you have the GPU recognized \n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "nQEoDjlMtCRG",
        "outputId": "e102f8da-8fcc-4bbe-ed9b-ff028bf70445",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jul 20 07:20:54 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Git clone COMP9312 repo"
      ],
      "metadata": {
        "id": "Lt3wfx8tbzRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/COMP9312\n",
        "!git clone https://github.com/naghamghanim/data.git"
      ],
      "metadata": {
        "id": "CSqBYb05SgZI",
        "outputId": "f3db500c-6ccc-493a-f300-0863b6860cdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'data'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 43 (delta 13), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (43/43), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies"
      ],
      "metadata": {
        "id": "s3eKx0pTcCA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "8Gw8kbhjXHGL",
        "outputId": "23946d74-160c-4444-9a49-6c434cbb8347",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 66.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 55.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 13.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training"
      ],
      "metadata": {
        "id": "02rBEWrjcMLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make output directory\n",
        "!mkdir /content/output/"
      ],
      "metadata": {
        "id": "6QHS5JcQsoHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import argparse\n",
        "sys.path.append('/content/data/')"
      ],
      "metadata": {
        "id": "5lzJLbXzSmvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from comp9312.classify.train import main"
      ],
      "metadata": {
        "id": "yqGx1LLfV4c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args_dict = {\n",
        "    \"output_path\": \"/content/output/\",\n",
        "    \"train_path\": \"/content/data/data/AJGT-5000/train.csv\",\n",
        "    \"test_path\": \"/content/data/data/AJGT-5000/test1.csv\",\n",
        "    \"val_path\": \"/content/data/data/AJGT-5000/val.csv\",\n",
        "    \"seed\": 1,\n",
        "    \"max_epochs\": 10,\n",
        "    \"batch_size\": 8,\n",
        "    \"bert_model\": \"avichr/heBERT\",\n",
        "    \"num_workers\": 1,\n",
        "    \"gpus\": [0],\n",
        "    \"learning_rate\": 0.00001\n",
        "}\n",
        "args = argparse.Namespace()\n",
        "args.__dict__ = args_dict"
      ],
      "metadata": {
        "id": "AwmPOruvWSS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(args)"
      ],
      "metadata": {
        "id": "JJV8V5GuYKRS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49e8a183-f997-4fff-a042-ddca3e53eadb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at avichr/heBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:05\tEpoch 0 | Batch 10/438 | Timestep 10 | Loss 0.696827\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:06\tEpoch 0 | Batch 20/438 | Timestep 20 | Loss 0.653247\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:08\tEpoch 0 | Batch 30/438 | Timestep 30 | Loss 0.718707\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:09\tEpoch 0 | Batch 40/438 | Timestep 40 | Loss 0.745690\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:11\tEpoch 0 | Batch 50/438 | Timestep 50 | Loss 0.616073\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:12\tEpoch 0 | Batch 60/438 | Timestep 60 | Loss 0.571549\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:14\tEpoch 0 | Batch 70/438 | Timestep 70 | Loss 0.547208\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:15\tEpoch 0 | Batch 80/438 | Timestep 80 | Loss 0.784110\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:16\tEpoch 0 | Batch 90/438 | Timestep 90 | Loss 0.682880\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:18\tEpoch 0 | Batch 100/438 | Timestep 100 | Loss 0.770676\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:19\tEpoch 0 | Batch 110/438 | Timestep 110 | Loss 0.587234\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:21\tEpoch 0 | Batch 120/438 | Timestep 120 | Loss 0.694520\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:22\tEpoch 0 | Batch 130/438 | Timestep 130 | Loss 0.637939\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:24\tEpoch 0 | Batch 140/438 | Timestep 140 | Loss 0.656459\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:26\tEpoch 0 | Batch 150/438 | Timestep 150 | Loss 0.765422\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:27\tEpoch 0 | Batch 160/438 | Timestep 160 | Loss 0.767385\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:29\tEpoch 0 | Batch 170/438 | Timestep 170 | Loss 0.625811\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:30\tEpoch 0 | Batch 180/438 | Timestep 180 | Loss 0.672906\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:32\tEpoch 0 | Batch 190/438 | Timestep 190 | Loss 0.569113\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:33\tEpoch 0 | Batch 200/438 | Timestep 200 | Loss 0.577976\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:35\tEpoch 0 | Batch 210/438 | Timestep 210 | Loss 0.650532\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:36\tEpoch 0 | Batch 220/438 | Timestep 220 | Loss 0.589582\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:37\tEpoch 0 | Batch 230/438 | Timestep 230 | Loss 0.736147\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:39\tEpoch 0 | Batch 240/438 | Timestep 240 | Loss 0.879702\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:40\tEpoch 0 | Batch 250/438 | Timestep 250 | Loss 0.781864\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:42\tEpoch 0 | Batch 260/438 | Timestep 260 | Loss 0.737607\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:43\tEpoch 0 | Batch 270/438 | Timestep 270 | Loss 0.610967\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:45\tEpoch 0 | Batch 280/438 | Timestep 280 | Loss 0.718705\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:47\tEpoch 0 | Batch 290/438 | Timestep 290 | Loss 0.565352\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:48\tEpoch 0 | Batch 300/438 | Timestep 300 | Loss 0.634460\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:50\tEpoch 0 | Batch 310/438 | Timestep 310 | Loss 0.650196\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:51\tEpoch 0 | Batch 320/438 | Timestep 320 | Loss 0.630666\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:53\tEpoch 0 | Batch 330/438 | Timestep 330 | Loss 0.647332\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:54\tEpoch 0 | Batch 340/438 | Timestep 340 | Loss 0.653280\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:56\tEpoch 0 | Batch 350/438 | Timestep 350 | Loss 0.640634\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:57\tEpoch 0 | Batch 360/438 | Timestep 360 | Loss 0.695748\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:01:59\tEpoch 0 | Batch 370/438 | Timestep 370 | Loss 0.738728\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:01\tEpoch 0 | Batch 380/438 | Timestep 380 | Loss 0.715120\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:02\tEpoch 0 | Batch 390/438 | Timestep 390 | Loss 0.611201\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:04\tEpoch 0 | Batch 400/438 | Timestep 400 | Loss 0.636276\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:05\tEpoch 0 | Batch 410/438 | Timestep 410 | Loss 0.753725\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:07\tEpoch 0 | Batch 420/438 | Timestep 420 | Loss 0.610504\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:08\tEpoch 0 | Batch 430/438 | Timestep 430 | Loss 0.680894\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:09\t** Evaluating on validation dataset **\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:12\t\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.80      0.02      0.04       195\n",
            "    Positive       0.61      1.00      0.76       304\n",
            "\n",
            "    accuracy                           0.62       499\n",
            "   macro avg       0.71      0.51      0.40       499\n",
            "weighted avg       0.69      0.62      0.48       499\n",
            "\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:12\tEpoch 0 | Timestep 438 | Train Loss 0.672511 | Val Loss 0.646433 | F1 Micro 0.615230\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:12\t** Validation improved, evaluating test data **\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:17\t\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.62      0.01      0.02       414\n",
            "    Positive       0.59      0.99      0.74       587\n",
            "\n",
            "    accuracy                           0.59      1001\n",
            "   macro avg       0.61      0.50      0.38      1001\n",
            "weighted avg       0.60      0.59      0.44      1001\n",
            "\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:17\tEpoch 0 | Timestep 438 | Test Loss 0.656921 | F1 Micro 0.588412\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:17\tSaving checkpoint to /content/output/model.pt\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:22\tEpoch 1 | Batch 2/438 | Timestep 440 | Loss 0.711834\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:24\tEpoch 1 | Batch 12/438 | Timestep 450 | Loss 0.592411\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:25\tEpoch 1 | Batch 22/438 | Timestep 460 | Loss 0.555220\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:26\tEpoch 1 | Batch 32/438 | Timestep 470 | Loss 0.758657\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:28\tEpoch 1 | Batch 42/438 | Timestep 480 | Loss 0.616515\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:29\tEpoch 1 | Batch 52/438 | Timestep 490 | Loss 0.737986\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:31\tEpoch 1 | Batch 62/438 | Timestep 500 | Loss 0.725799\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:32\tEpoch 1 | Batch 72/438 | Timestep 510 | Loss 0.730502\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:33\tEpoch 1 | Batch 82/438 | Timestep 520 | Loss 0.668201\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:35\tEpoch 1 | Batch 92/438 | Timestep 530 | Loss 0.722738\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:36\tEpoch 1 | Batch 102/438 | Timestep 540 | Loss 0.593054\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:38\tEpoch 1 | Batch 112/438 | Timestep 550 | Loss 0.646566\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:39\tEpoch 1 | Batch 122/438 | Timestep 560 | Loss 0.687744\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:41\tEpoch 1 | Batch 132/438 | Timestep 570 | Loss 0.602589\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:43\tEpoch 1 | Batch 142/438 | Timestep 580 | Loss 0.670150\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:44\tEpoch 1 | Batch 152/438 | Timestep 590 | Loss 0.606071\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:46\tEpoch 1 | Batch 162/438 | Timestep 600 | Loss 0.529751\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:47\tEpoch 1 | Batch 172/438 | Timestep 610 | Loss 0.591773\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:49\tEpoch 1 | Batch 182/438 | Timestep 620 | Loss 0.639666\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:50\tEpoch 1 | Batch 192/438 | Timestep 630 | Loss 0.630947\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:51\tEpoch 1 | Batch 202/438 | Timestep 640 | Loss 0.666452\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:53\tEpoch 1 | Batch 212/438 | Timestep 650 | Loss 0.684216\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:54\tEpoch 1 | Batch 222/438 | Timestep 660 | Loss 0.580938\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:56\tEpoch 1 | Batch 232/438 | Timestep 670 | Loss 0.674904\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:57\tEpoch 1 | Batch 242/438 | Timestep 680 | Loss 0.615526\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:02:59\tEpoch 1 | Batch 252/438 | Timestep 690 | Loss 0.686222\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:03:00\tEpoch 1 | Batch 262/438 | Timestep 700 | Loss 0.684142\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:03:02\tEpoch 1 | Batch 272/438 | Timestep 710 | Loss 0.685301\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:03:03\tEpoch 1 | Batch 282/438 | Timestep 720 | Loss 0.577456\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:03:05\tEpoch 1 | Batch 292/438 | Timestep 730 | Loss 0.486888\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:03:06\tEpoch 1 | Batch 302/438 | Timestep 740 | Loss 0.662075\n",
            "INFO\tcomp9312.classify.trainer\tWed, 20 Jul 2022 09:03:08\tEpoch 1 | Batch 312/438 | Timestep 750 | Loss 0.636813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kOT82AyWsuav"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}